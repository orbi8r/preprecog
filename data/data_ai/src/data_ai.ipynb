{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c480a38f",
   "metadata": {},
   "source": [
    "# Task 0: AI Data Pipeline (Class 2 & Class 3)\n",
    "\n",
    "Generates synthetic datasets using the Gemini API, mirroring the human dataset's distribution.\n",
    "\n",
    "- **Class 2 (Standard AI):** Minimalist topic-only prompts across Gemini 3 Flash, Gemini 2.5 Flash, and Gemini 2.5 Flash Lite.\n",
    "- **Class 3 (Imposter AI):** Roleplay prompts instructing the model to mimic a specific author's style and era.\n",
    "\n",
    "AI output (800–1000 words) is cleaned and chunked with the same 100–200 word pipeline used for human data, preserving a comparable \"choppy and incomplete\" feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "from google import genai\n",
    "from google.api_core import exceptions\n",
    "\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "    sys.stderr.reconfigure(encoding='utf-8')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=r\"You are using a Python version .* google.api_core\", category=FutureWarning)\n",
    "os.environ.setdefault(\"GRPC_VERBOSITY\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "BASE_DIR = os.path.abspath(\"..\")\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))\n",
    "ENV_PATH = os.path.join(PROJECT_ROOT, \".env\")\n",
    "\n",
    "DATA_HUMAN_DIR = os.path.join(PROJECT_ROOT, \"data\", \"data_human\", \"processed\")\n",
    "INPUT_FILE = os.path.join(DATA_HUMAN_DIR, \"human_class1.parquet\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"WARNING: GEMINI_API_KEY not found in .env files. AI calls will fail.\")\n",
    "    client = None\n",
    "else:\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c66b9d",
   "metadata": {},
   "source": [
    "## Cleaning & Chunking\n",
    "\n",
    "Same strict 100–200 word chunking pipeline as the human data. Curly-bracket parsing extracts only the essay body from API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "def deep_clean_text(text):\n",
    "    if text is None: return \"\"\n",
    "    text = re.sub(r'\\[.*?\\]', '', text, flags=re.DOTALL)\n",
    "    text = text.replace('_', '')\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def get_chunks(text, min_w=100, max_w=200):\n",
    "    ensure_nltk_resources()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    current_chunk = []\n",
    "    current_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        w_count = len(sentence.split())\n",
    "\n",
    "        if w_count > max_w:\n",
    "            if current_count >= min_w:\n",
    "                yield \" \".join(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_count = 0\n",
    "            continue\n",
    "\n",
    "        if current_count + w_count <= max_w:\n",
    "            current_chunk.append(sentence)\n",
    "            current_count += w_count\n",
    "            if current_count >= 150:\n",
    "                yield \" \".join(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "        else:\n",
    "            if current_count >= min_w:\n",
    "                yield \" \".join(current_chunk)\n",
    "            current_chunk = [sentence]\n",
    "            current_count = w_count\n",
    "\n",
    "    if current_count >= min_w:\n",
    "        yield \" \".join(current_chunk)\n",
    "\n",
    "def analyze_chunk(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = text.split()\n",
    "    sent_count = len(sentences)\n",
    "    return {\"word_count\": len(words), \"avg_sent_len\": len(words) / sent_count if sent_count else 0}\n",
    "\n",
    "def parse_curly_content(text):\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return text[start+1:end].strip()\n",
    "    else:\n",
    "        print(\"Warning: AI forgot brackets. Taking raw text.\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e73dd",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "- **Class 2:** Topic-only prompt requesting a comprehensive essay.\n",
    "- **Class 3:** Roleplay prompt mimicking a specific author's style, vocabulary, and era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_PROFILES = {\n",
    "    \"Bacon\": {\n",
    "        \"year\": \"1625\",\n",
    "        \"style\": \"archaic, aphoristic, heavy use of semi-colons, logical, authoritative, Renaissance English\",\n",
    "        \"context\": \"The Essays or Counsels, Civil and Moral\"\n",
    "    },\n",
    "    \"Emerson\": {\n",
    "        \"year\": \"1841\",\n",
    "        \"style\": \"transcendentalist, poetic, flowery, metaphor-heavy, focus on individualism and nature\",\n",
    "        \"context\": \"Essays: First Series\"\n",
    "    },\n",
    "    \"James\": {\n",
    "        \"year\": \"1907\",\n",
    "        \"style\": \"pragmatic, academic but accessible, first-person ('I'), focus on practical consequences\",\n",
    "        \"context\": \"Lectures on Pragmatism\"\n",
    "    },\n",
    "    \"Russell\": {\n",
    "        \"year\": \"1912\",\n",
    "        \"style\": \"analytic, dry, precise, logical, focus on definitions and epistemology\",\n",
    "        \"context\": \"The Problems of Philosophy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def build_prompt_class_2(topic):\n",
    "    return f\"\"\"\n",
    "    Write a comprehensive essay on the topic of \"{topic}\".\n",
    "    \n",
    "    Constraints:\n",
    "    1. Length: Approximately 800-1000 words (about 40-60 sentences).\n",
    "    2. Format: Enclose the ENTIRE essay body inside curly brackets {{ }}. \n",
    "       Example: {{ The essay starts here... and ends here. }}\n",
    "    3. Do not put introductions like \"Here is your essay\" inside the brackets.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_class_3(topic, author):\n",
    "    profile = AUTHOR_PROFILES.get(author, AUTHOR_PROFILES[\"Emerson\"])\n",
    "    return f\"\"\"\n",
    "    Roleplay Task: You are {author}, writing in the year {profile['year']}.\n",
    "    \n",
    "    Task: Write a new chapter for your book \"{profile['context']}\" on the topic of \"{topic}\".\n",
    "    \n",
    "    Style Guidelines:\n",
    "    - Imitate this style: {profile['style']}.\n",
    "    - Use the vocabulary and sentence structure typical of {profile['year']}.\n",
    "    - Do NOT copy existing text, but generate new thoughts in that exact voice.\n",
    "    \n",
    "    Constraints:\n",
    "    1. Length: Approximately 800-1000 words (about 40-60 sentences).\n",
    "    2. Format: Enclose the ENTIRE essay text inside curly brackets {{ }}.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ed1fc",
   "metadata": {},
   "source": [
    "## API Interaction\n",
    "\n",
    "Handles retries and rate-limiting (429 errors) with exponential backoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcefc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ai_api(prompt, max_retries=5):\n",
    "    if client is None:\n",
    "        raise ValueError(\"API Key missing or client not initialized.\")\n",
    "\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.models.generate_content(model=MODEL_NAME, contents=prompt)\n",
    "            if hasattr(response, \"text\") and response.text:\n",
    "                return response.text\n",
    "            return str(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            err_str = str(e)\n",
    "            if \"429\" in err_str or \"ResourceExhausted\" in str(type(e)) or \"Quota exceeded\" in err_str:\n",
    "                wait_time = 30.0\n",
    "                match = re.search(r\"retry in ([0-9\\.]+)s\", err_str)\n",
    "                if match:\n",
    "                    wait_time = float(match.group(1)) + 10.0\n",
    "                print(f\"Rate Limit Hit. Pausing for {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "            attempt += 1\n",
    "            if attempt >= max_retries:\n",
    "                print(f\"Failed after {max_retries} attempts. Error: {e}\")\n",
    "                return \"{}\"\n",
    "\n",
    "            backoff_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "            print(f\"API Error: {e}. Retrying in {backoff_time:.1f}s...\")\n",
    "            time.sleep(backoff_time)\n",
    "\n",
    "def get_existing_data(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            return pd.read_parquet(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing parquet: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_incremental(df_new, filepath):\n",
    "    if df_new.empty: return\n",
    "    df_existing = get_existing_data(filepath)\n",
    "    if not df_existing.empty:\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "    df_combined.to_parquet(filepath)\n",
    "    print(f\"Saved {len(df_new)} new rows to {filepath}. Total rows: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb84fb",
   "metadata": {},
   "source": [
    "## Balanced Generation Loop\n",
    "\n",
    "Queries the human dataset distribution and generates AI samples to match the exact `(Topic, Author)` pair weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652af829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_dataset(human_df, class_label, target_total=500, output_filename=\"ai_data.parquet\"):\n",
    "    print(f\"\\n--- Generating Class {class_label} ({'Standard' if class_label==2 else 'Imposter'}) ---\")\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "    existing_df = get_existing_data(output_path)\n",
    "    current_count = len(existing_df) if not existing_df.empty else 0\n",
    "\n",
    "    if not existing_df.empty:\n",
    "        print(f\"Found existing data with {current_count} rows.\")\n",
    "\n",
    "    if current_count >= target_total:\n",
    "        print(f\"Target of {target_total} reached. Skipping generation.\")\n",
    "        return existing_df\n",
    "\n",
    "    distribution = human_df.groupby(['topic', 'feature_cache_author']).size().reset_index(name='count')\n",
    "    pop_topics = distribution['topic'].tolist()\n",
    "    pop_authors = distribution['feature_cache_author'].tolist()\n",
    "    pop_weights = distribution['count'].tolist()\n",
    "\n",
    "    generated_buffer = []\n",
    "\n",
    "    while current_count < target_total:\n",
    "        selection = random.choices(\n",
    "            population=list(zip(pop_topics, pop_authors)),\n",
    "            weights=pop_weights,\n",
    "            k=1\n",
    "        )[0]\n",
    "\n",
    "        topic, author = selection\n",
    "\n",
    "        if class_label == 2:\n",
    "            prompt = build_prompt_class_2(topic)\n",
    "            persona_used = \"Generic_AI\"\n",
    "        else:\n",
    "            prompt = build_prompt_class_3(topic, author)\n",
    "            persona_used = f\"Imposter_{author}\"\n",
    "\n",
    "        print(f\"Generating ({current_count + 1}/{target_total}): {topic} [{persona_used}]...\")\n",
    "\n",
    "        gen_attempt = 0\n",
    "        max_gen_attempts = 4\n",
    "        raw_output = None\n",
    "\n",
    "        while gen_attempt < max_gen_attempts:\n",
    "            try:\n",
    "                raw_output = call_ai_api(prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Generation failed: {e}\")\n",
    "                gen_attempt += 1\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "\n",
    "        if raw_output is None:\n",
    "            continue\n",
    "\n",
    "        essay_body = parse_curly_content(raw_output)\n",
    "        clean_text = deep_clean_text(essay_body)\n",
    "\n",
    "        if not clean_text or len(clean_text) < 50:\n",
    "            continue\n",
    "\n",
    "        chunks = get_chunks(clean_text)\n",
    "        current_chunks = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            features = analyze_chunk(chunk)\n",
    "            if features['word_count'] < 100 or features['word_count'] > 200:\n",
    "                continue\n",
    "\n",
    "            new_row = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"class\": class_label,\n",
    "                \"topic\": topic,\n",
    "                \"text\": chunk,\n",
    "                \"feature_cache\": {\n",
    "                    \"author\": MODEL_NAME,\n",
    "                    \"persona_mimicked\": persona_used,\n",
    "                    \"word_count\": features[\"word_count\"],\n",
    "                    \"avg_sent_length\": features[\"avg_sent_len\"]\n",
    "                }\n",
    "            }\n",
    "            current_chunks.append(new_row)\n",
    "\n",
    "        generated_buffer.extend(current_chunks)\n",
    "\n",
    "        if len(generated_buffer) >= 5:\n",
    "            df_new = pd.DataFrame(generated_buffer)\n",
    "            save_incremental(df_new, output_path)\n",
    "            current_count += len(generated_buffer)\n",
    "            generated_buffer = []\n",
    "\n",
    "        if current_count >= target_total: break\n",
    "\n",
    "    if generated_buffer:\n",
    "        df_new = pd.DataFrame(generated_buffer)\n",
    "        save_incremental(df_new, output_path)\n",
    "\n",
    "    return get_existing_data(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3050d",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file not found: {INPUT_FILE}\\nRun the Human Data Notebook first!\")\n",
    "    else:\n",
    "        df_human = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "        def get_author(fc):\n",
    "            if isinstance(fc, dict): return fc.get('author')\n",
    "            return None\n",
    "\n",
    "        df_human['feature_cache_author'] = df_human['feature_cache'].apply(get_author)\n",
    "        df_human = df_human.dropna(subset=['feature_cache_author'])\n",
    "\n",
    "        print(f\"Loaded Human Data: {len(df_human)} rows. Author distribution:\")\n",
    "        print(df_human['feature_cache_author'].value_counts())\n",
    "\n",
    "        generate_balanced_dataset(df_human, class_label=2, target_total=500, output_filename=\"ai_class2.parquet\")\n",
    "        # generate_balanced_dataset(df_human, class_label=3, target_total=500, output_filename=\"ai_class3.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
