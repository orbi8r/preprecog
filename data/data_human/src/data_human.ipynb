{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296f9221",
   "metadata": {},
   "source": [
    "# Task 0: Human Data Pipeline (Class 1)\n",
    "\n",
    "Processes raw Project Gutenberg essays into a balanced parquet dataset.\n",
    "\n",
    "**Authors:** Francis Bacon (1597), Ralph Waldo Emerson (1841), William James (1907), Bertrand Russell (1912)\n",
    "\n",
    "**Pipeline:** Remove metadata/artifacts → Split into chapter blocks → Assign topics → Sentence-tokenize and chunk to 100–200 words → Balance to 125 samples per author → Export as `human_class1.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "ensure_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(\"..\")\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "BOOKS = [\n",
    "    {\"filename\": \"FrancisBacon.txt\", \"author\": \"Francis Bacon\", \"title\": \"The Essays\", \"source_identity\": \"Bacon\", \"allowed_topics\": None},\n",
    "    {\"filename\": \"RalphWaldo.txt\", \"author\": \"Ralph Waldo Emerson\", \"title\": \"Essays: First Series\", \"source_identity\": \"Emerson\", \"allowed_topics\": None},\n",
    "    {\"filename\": \"WilliamJames.txt\", \"author\": \"William James\", \"title\": \"Pragmatism\", \"source_identity\": \"James_1907\", \"allowed_topics\": None},\n",
    "    {\"filename\": \"BertrandRussell.txt\", \"author\": \"Bertrand Russell\", \"title\": \"The Problems of Philosophy\", \"source_identity\": \"Russell_1912\", \"allowed_topics\": None}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b38d1",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "Regex removal of Project Gutenberg headers/footers, `[Footnote]` markers, `_italics_` underscores, and excess whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_header_footer(text):\n",
    "    start_markers = [r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"]\n",
    "    end_markers = [r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"]\n",
    "    \n",
    "    start_pos = 0\n",
    "    end_pos = len(text)\n",
    "    \n",
    "    for marker in start_markers:\n",
    "        match = re.search(marker, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start_pos = match.end()\n",
    "            break\n",
    "            \n",
    "    for marker in end_markers:\n",
    "        match = re.search(marker, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            end_pos = match.start()\n",
    "            break\n",
    "            \n",
    "    return text[start_pos:end_pos].strip()\n",
    "\n",
    "def deep_clean_text(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text, flags=re.DOTALL)\n",
    "    text = text.replace('_', '')\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9048092",
   "metadata": {},
   "source": [
    "## Chapter Splitting\n",
    "\n",
    "Split each book into logical chapters using author-specific formatting patterns, preserving semantic context before chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chapters(text, author):\n",
    "    chapters = []\n",
    "\n",
    "    if \"Emerson\" in author:\n",
    "        titles = [\"HISTORY\", \"SELF-RELIANCE\", \"COMPENSATION\", \"SPIRITUAL LAWS\", \"LOVE\", \"FRIENDSHIP\", \"PRUDENCE\", \"HEROISM\", \"THE OVER-SOUL\", \"CIRCLES\", \"INTELLECT\", \"ART\"]\n",
    "        pattern = re.compile(r'(?:\\n\\r?|\\r\\n)\\s*(' + '|'.join([re.escape(t) for t in titles]) + r')\\s*(?:\\n\\r?|\\r\\n)', re.IGNORECASE)\n",
    "        parts = pattern.split(text)\n",
    "        if parts[0].strip(): chapters.append({'topic': 'Preface', 'text': parts[0].strip()})\n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                chapters.append({'topic': parts[i].strip().title(), 'text': parts[i+1].strip()})\n",
    "\n",
    "    elif \"Bacon\" in author:\n",
    "        pattern = re.compile(r'(^Of [A-Za-z \\-]+$)', re.MULTILINE)\n",
    "        parts = pattern.split(text)\n",
    "        if parts[0].strip(): chapters.append({'topic': 'Preface', 'text': parts[0].strip()})\n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts):\n",
    "                content = parts[i+1].strip()\n",
    "                if len(content.split()) > 50:\n",
    "                    chapters.append({'topic': parts[i].strip().replace(\"Of \", \"\").title(), 'text': content})\n",
    "\n",
    "    elif \"Russell\" in author:\n",
    "        russell_titles = [\"APPEARANCE AND REALITY\", \"THE EXISTENCE OF MATTER\", \"THE NATURE OF MATTER\", \"IDEALISM\", \"KNOWLEDGE BY ACQUAINTANCE\", \"ON INDUCTION\", \"ON OUR KNOWLEDGE OF GENERAL PRINCIPLES\", \"HOW A PRIORI KNOWLEDGE IS POSSIBLE\", \"THE WORLD OF UNIVERSALS\", \"ON OUR KNOWLEDGE OF UNIVERSALS\", \"ON INTUITIVE KNOWLEDGE\", \"TRUTH AND FALSEHOOD\", \"KNOWLEDGE ERROR AND PROBABLE OPINION\", \"THE LIMITS OF PHILOSOPHICAL KNOWLEDGE\", \"THE VALUE OF PHILOSOPHY\"]\n",
    "        for i, title in enumerate(russell_titles):\n",
    "            start_idx = text.find(title)\n",
    "            if start_idx == -1: continue\n",
    "            end_idx = text.find(russell_titles[i+1]) if i + 1 < len(russell_titles) else len(text)\n",
    "            if end_idx == -1: end_idx = len(text)\n",
    "            content = text[start_idx + len(title):end_idx].strip()\n",
    "            chapters.append({'topic': title.title(), 'text': content})\n",
    "\n",
    "    elif \"James\" in author:\n",
    "        james_titles = [\"The Present Dilemma in Philosophy\", \"What Pragmatism Means\", \"Some Metaphysical Problems Pragmatically Considered\", \"The One and the Many\", \"Pragmatism and Common Sense\", \"Pragmatism's Conception of Truth\", \"Pragmatism and Humanism\", \"Pragmatism and Religion\"]\n",
    "        pattern_str = '(' + '|'.join([re.escape(t) for t in james_titles]) + ')'\n",
    "        pattern = re.compile(pattern_str, re.IGNORECASE)\n",
    "        matches = list(pattern.finditer(text))\n",
    "        for i, match in enumerate(matches):\n",
    "            start_pos = match.end()\n",
    "            end_pos = matches[i+1].start() if i + 1 < len(matches) else len(text)\n",
    "            chapters.append({'topic': match.group(1), 'text': text[start_pos:end_pos].strip()})\n",
    "\n",
    "    else:\n",
    "        chapters.append({'topic': 'Unknown', 'text': text})\n",
    "\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b04a8a",
   "metadata": {},
   "source": [
    "## Chunking (100–200 Words)\n",
    "\n",
    "Sentence-tokenize and accumulate chunks at 100–200 words (150 target). This strict window removes paragraph length as a discriminating feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44089db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text, min_w=100, max_w=200):\n",
    "    ensure_nltk_resources()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    current_chunk = []\n",
    "    current_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        w_count = len(sentence.split())\n",
    "\n",
    "        if w_count > max_w:\n",
    "            if current_count >= min_w:\n",
    "                yield \" \".join(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_count = 0\n",
    "            continue\n",
    "\n",
    "        if current_count + w_count <= max_w:\n",
    "            current_chunk.append(sentence)\n",
    "            current_count += w_count\n",
    "            if current_count >= 150:\n",
    "                yield \" \".join(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "        else:\n",
    "            if current_count >= min_w:\n",
    "                yield \" \".join(current_chunk)\n",
    "            current_chunk = [sentence]\n",
    "            current_count = w_count\n",
    "\n",
    "    if current_count >= min_w:\n",
    "        yield \" \".join(current_chunk)\n",
    "\n",
    "def analyze_chunk(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = text.split()\n",
    "    sent_count = len(sentences)\n",
    "    return {\"word_count\": len(words), \"avg_sent_len\": len(words) / sent_count if sent_count else 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcfd22d",
   "metadata": {},
   "source": [
    "## Topic Mapping\n",
    "\n",
    "Map chapter titles to six cohesive topics: Ethics & Conduct, General Philosophy, Mind & Knowledge, Religion & Spirit, Society & Politics, Truth & Reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_MAP = {\n",
    "    \"Truth\": \"Truth & Reality\", \"Truth And Falsehood\": \"Truth & Reality\", \"Pragmatism'S Conception Of Truth\": \"Truth & Reality\", \"The Conception Of Truth\": \"Truth & Reality\", \"Appearance And Reality\": \"Truth & Reality\", \"The Existence Of Matter\": \"Truth & Reality\", \"The Nature Of Matter\": \"Truth & Reality\", \"What Pragmatism Means\": \"Truth & Reality\",\n",
    "    \"Intellect\": \"Mind & Knowledge\", \"Knowledge By Acquaintance\": \"Mind & Knowledge\", \"On Our Knowledge Of General Principles\": \"Mind & Knowledge\", \"How A Priori Knowledge Is Possible\": \"Mind & Knowledge\", \"On Intuitive Knowledge\": \"Mind & Knowledge\", \"The Limits Of Philosophical Knowledge\": \"Mind & Knowledge\", \"The World Of Universals\": \"Mind & Knowledge\", \"Circles\": \"Mind & Knowledge\", \"The Present Dilemma In Philosophy\": \"Mind & Knowledge\",\n",
    "    \"Goodness And Goodness Of Nature\": \"Ethics & Conduct\", \"Adversity\": \"Ethics & Conduct\", \"Boldness\": \"Ethics & Conduct\", \"Envy\": \"Ethics & Conduct\", \"Revenge\": \"Ethics & Conduct\", \"Simulation And Dissimulation\": \"Ethics & Conduct\", \"Prudence\": \"Ethics & Conduct\", \"Heroism\": \"Ethics & Conduct\", \"Compensation\": \"Ethics & Conduct\", \"Self-Reliance\": \"Ethics & Conduct\",\n",
    "    \"Judicature\": \"Society & Politics\", \"Empire\": \"Society & Politics\", \"Great Place\": \"Society & Politics\", \"Innovations\": \"Society & Politics\", \"Marriage And Single Life\": \"Society & Politics\", \"Parents And Children\": \"Society & Politics\", \"Friendship\": \"Society & Politics\", \"Manners\": \"Society & Politics\", \"Seditions And Troubles\": \"Society & Politics\", \"The True Greatness Kingdoms And Estates\": \"Society & Politics\",\n",
    "    \"Unity In Religion\": \"Religion & Spirit\", \"Atheism\": \"Religion & Spirit\", \"Superstition\": \"Religion & Spirit\", \"The Over-Soul\": \"Religion & Spirit\", \"Spiritual Laws\": \"Religion & Spirit\", \"Pragmatism And Religion\": \"Religion & Spirit\", \"Some Metaphysical Problems Pragmatically Considered\": \"Religion & Spirit\"\n",
    "}\n",
    "\n",
    "def map_topic(row):\n",
    "    key = str(row['topic']).strip().title()\n",
    "    return TOPIC_MAP.get(key, \"General Philosophy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056cb231",
   "metadata": {},
   "source": [
    "## Process and Export\n",
    "\n",
    "Read → clean → split → chunk → balance (125 per author) → save as `human_class1.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for book in BOOKS:\n",
    "    filepath = os.path.join(RAW_DIR, book[\"filename\"])\n",
    "    print(f\"Processing {book['title']}...\")\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f: content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        continue\n",
    "\n",
    "    clean_content = clean_gutenberg_header_footer(content)\n",
    "    chapters = split_into_chapters(clean_content, book[\"author\"])\n",
    "\n",
    "    for chapter in chapters:\n",
    "        clean_body = deep_clean_text(chapter['text'])\n",
    "        chunks = get_chunks(clean_body)\n",
    "        for chunk_text in chunks:\n",
    "            features = analyze_chunk(chunk_text)\n",
    "            all_chunks.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"text\": chunk_text,\n",
    "                \"class\": 1,\n",
    "                \"topic\": chapter['topic'],\n",
    "                \"origin_ref\": f\"{book['title']}/{chapter['topic']}\",\n",
    "                \"feature_cache\": {\n",
    "                    \"author\": book[\"source_identity\"].split('_')[0],\n",
    "                    \"book_title\": book[\"title\"],\n",
    "                    \"word_count\": features[\"word_count\"],\n",
    "                    \"avg_sent_length\": features[\"avg_sent_len\"]\n",
    "                }\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(all_chunks)\n",
    "\n",
    "if not df.empty:\n",
    "    final_dfs = []\n",
    "    df['author_key'] = df['feature_cache'].apply(lambda x: x['author'])\n",
    "\n",
    "    for author in df['author_key'].unique():\n",
    "        sub_df = df[df['author_key'] == author]\n",
    "        if len(sub_df) > 125:\n",
    "            final_dfs.append(sub_df.sample(n=125, random_state=42))\n",
    "        else:\n",
    "            final_dfs.append(sub_df)\n",
    "\n",
    "    final_df = pd.concat(final_dfs, ignore_index=True)\n",
    "    final_df['topic'] = final_df.apply(map_topic, axis=1)\n",
    "    final_df = final_df[['id', 'class', 'topic', 'text', 'feature_cache', 'origin_ref']]\n",
    "\n",
    "    out_path = os.path.join(PROCESSED_DIR, \"human_class1.parquet\")\n",
    "    final_df.to_parquet(out_path)\n",
    "    print(f\"Saved {len(final_df)} clean rows to {out_path}\")\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"No data generated.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
