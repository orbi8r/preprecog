{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632fc7b5",
   "metadata": {},
   "source": [
    "# Per-Row Feature Enrichment\n",
    "Computes all 26 features per row (lexical, syntactic, punctuation, function-word PCA) and writes enriched data.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d71c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "WORD_RE = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return [w.lower() for w in WORD_RE.findall(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae207c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f21052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read merged data.parquet (W:\\Programming\\PKOG\\preprecogclean\\data\\data.parquet) with 1508 rows\n"
     ]
    }
   ],
   "source": [
    "candidates = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]\n",
    "\n",
    "def find_path(rel):\n",
    "    for base in candidates:\n",
    "        p = base.joinpath(rel)\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "merged_path = find_path(Path('data').joinpath('data.parquet'))\n",
    "if merged_path:\n",
    "    df = pd.read_parquet(merged_path)\n",
    "    print(f\"Read merged data.parquet ({merged_path}) with {len(df)} rows\")\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    print(\"No data available to enrich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8952f7",
   "metadata": {},
   "source": [
    "## Lexical metrics per row\n",
    "TTR, hapax, MTLD, sentence length std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d123156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(tokens):\n",
    "    N = len(tokens)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    return len(set(tokens)) / N\n",
    "\n",
    "\n",
    "def hapax_in_sample(tokens, sample_size=5000, seed=None):\n",
    "    N = len(tokens)\n",
    "    if N == 0:\n",
    "        return 0\n",
    "    if N <= sample_size:\n",
    "        sample = tokens\n",
    "    else:\n",
    "        rng = random.Random(seed)\n",
    "        start = rng.randint(0, N - sample_size)\n",
    "        sample = tokens[start:start+sample_size]\n",
    "    freq = Counter(sample)\n",
    "    return sum(1 for w,c in freq.items() if c == 1)\n",
    "\n",
    "\n",
    "def mtld_calc(tokens, ttr_threshold=0.72):\n",
    "    def mtld_single_pass(token_list):\n",
    "        factor_count = 0\n",
    "        token_count = 0\n",
    "        types = set()\n",
    "        for w in token_list:\n",
    "            token_count += 1\n",
    "            types.add(w)\n",
    "            ttr = len(types) / token_count\n",
    "            if ttr <= ttr_threshold:\n",
    "                factor_count += 1\n",
    "                token_count = 0\n",
    "                types = set()\n",
    "        if token_count > 0:\n",
    "            ttr = len(types) / token_count if token_count else 0\n",
    "            partial = (1 - ttr) / (1 - ttr_threshold) if (1 - ttr_threshold) != 0 else 0\n",
    "            factor_count += partial\n",
    "        return (len(token_list) / factor_count) if factor_count != 0 else float('inf')\n",
    "\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    forward = mtld_single_pass(tokens)\n",
    "    backward = mtld_single_pass(list(reversed(tokens)))\n",
    "    return (forward + backward) / 2\n",
    "\n",
    "\n",
    "def sentence_length_std(text):\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    if not sents:\n",
    "        return 0.0\n",
    "    lengths = [len(tokenize(s)) for s in sents]\n",
    "    if not lengths:\n",
    "        return 0.0\n",
    "    mean = sum(lengths)/len(lengths)\n",
    "    var = sum((l-mean)**2 for l in lengths)/len(lengths)\n",
    "    return math.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6629357",
   "metadata": {},
   "source": [
    "## Syntactic metrics per row\n",
    "Adj/noun ratio, tree depth, FK grade, discourse density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5658ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "DISCOURSE_MARKERS = {\n",
    "    \"however\", \"therefore\", \"consequently\", \"furthermore\", \"moreover\", \"nevertheless\",\n",
    "    \"thus\", \"hence\", \"accordingly\", \"subsequently\", \"conversely\", \"meanwhile\",\n",
    "    \"nonetheless\", \"notwithstanding\", \"additionally\", \"alternatively\", \"undoubtedly\",\n",
    "    \"specifically\", \"similarly\", \"finally\", \"indeed\"\n",
    "}\n",
    "\n",
    "def get_tree_depth(token):\n",
    "    if not list(token.children):\n",
    "        return 1\n",
    "    return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "\n",
    "\n",
    "def calculate_average_depth(doc):\n",
    "    depths = []\n",
    "    for sent in doc.sents:\n",
    "        depths.append(get_tree_depth(sent.root))\n",
    "    return sum(depths) / len(depths) if depths else 0\n",
    "\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = len(re.findall(r'[aeiouy]+', word))\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    return max(1, count)\n",
    "\n",
    "\n",
    "def flesch_kincaid(doc):\n",
    "    n_words = len([t for t in doc if not t.is_punct])\n",
    "    n_sents = len(list(doc.sents))\n",
    "    n_syllables = sum(syllable_count(t.text) for t in doc if not t.is_punct)\n",
    "    if n_words == 0 or n_sents == 0:\n",
    "        return 0\n",
    "    return 0.39 * (n_words / n_sents) + 11.8 * (n_syllables / n_words) - 15.59\n",
    "\n",
    "\n",
    "def analyze_syntax(text):\n",
    "    if not text:\n",
    "        return {\n",
    "            \"adj_noun_ratio\": 0,\n",
    "            \"tree_depth\": 0,\n",
    "            \"fk_grade\": 0,\n",
    "            \"discourse_density_per_100_words\": 0\n",
    "        }\n",
    "    doc = nlp(text)\n",
    "    adjs = len([t for t in doc if t.pos_ == \"ADJ\"])\n",
    "    nouns = len([t for t in doc if t.pos_ == \"NOUN\"])\n",
    "    adj_noun_ratio = adjs / nouns if nouns > 0 else 0\n",
    "    avg_depth = calculate_average_depth(doc)\n",
    "    fk_grade = flesch_kincaid(doc)\n",
    "    discourse_count = len([t for t in doc if t.lower_ in DISCOURSE_MARKERS])\n",
    "    n_words = len([t for t in doc if not t.is_punct])\n",
    "    discourse_density_per_100_words = (discourse_count / n_words) * 100 if n_words > 0 else 0\n",
    "    return {\n",
    "        \"adj_noun_ratio\": adj_noun_ratio,\n",
    "        \"tree_depth\": avg_depth,\n",
    "        \"fk_grade\": fk_grade,\n",
    "        \"discourse_density_per_100_words\": discourse_density_per_100_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ed049",
   "metadata": {},
   "source": [
    "## Punctuation densities and function-word PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a53e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_NAME_MAP = {\n",
    "    ';': 'semicolon',\n",
    "    ':': 'colon',\n",
    "    '!': 'exclamation',\n",
    "    '?': 'question',\n",
    "    '-': 'hyphen',\n",
    "    '\\u2014': 'emdash',\n",
    "    '*': 'asterisk',\n",
    "    \"'\": 'apos',\n",
    "    '\\u2019': 'apos_curly',\n",
    "    '(': 'paren_open',\n",
    "    '\"': 'quote'\n",
    "}\n",
    "PUNCT_SYMBOLS = list(PUNCT_NAME_MAP.keys())\n",
    "PUNCT_NAMES = list(PUNCT_NAME_MAP.values())\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None, max_features=100)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"No data to process for PCA/punctuation\")\n",
    "else:\n",
    "    dtm = vectorizer.fit_transform(df['text'].fillna('').astype(str))\n",
    "    all_words = vectorizer.get_feature_names_out()\n",
    "    std_stops = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    function_indices = [i for i, w in enumerate(all_words) if w in std_stops]\n",
    "    if len(function_indices) < 20:\n",
    "        function_indices = list(range(min(50, len(all_words))))\n",
    "    else:\n",
    "        function_indices = function_indices[:50]\n",
    "\n",
    "    if len(function_indices) == 0:\n",
    "        freq_matrix = dtm.toarray()\n",
    "    else:\n",
    "        function_word_dtm = dtm[:, function_indices]\n",
    "        row_sums = np.array(function_word_dtm.sum(axis=1)).ravel()\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        freq_matrix = function_word_dtm.toarray() / row_sums[:, None]\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    try:\n",
    "        components = pca.fit_transform(freq_matrix)\n",
    "    except Exception as e:\n",
    "        components = np.zeros((len(df), 2))\n",
    "\n",
    "    df['function_word_pca_dim1'] = components[:, 0]\n",
    "    df['function_word_pca_dim2'] = components[:, 1]\n",
    "\n",
    "    def punct_density(text):\n",
    "        text = text or ''\n",
    "        L = len(text)\n",
    "        if L == 0:\n",
    "            return {name: 0 for name in PUNCT_NAMES}\n",
    "        c = Counter(text)\n",
    "        return {name: (c[sym] / L) * 1000 for sym, name in PUNCT_NAME_MAP.items()}\n",
    "\n",
    "    punct_df = df['text'].apply(lambda x: pd.Series(punct_density(x)))\n",
    "    for name in PUNCT_NAMES:\n",
    "        col = f\"punct_{name}\"\n",
    "        df[col] = punct_df[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d906ef",
   "metadata": {},
   "source": [
    "## Assemble feature cache and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d90ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cache updated for all rows\n",
      "Lexical: n_tokens, n_types, ttr, hapax_5k, mtld, sent_len_std\n",
      "Syntactic: adj_noun_ratio, tree_depth, fk_grade, discourse_density_per_100_words\n",
      "Stylometric: function_word_pca (dim1, dim2)\n",
      "Punctuation: 11 punct_* features\n"
     ]
    }
   ],
   "source": [
    "if 'feature_cache' not in df.columns:\n",
    "    df['feature_cache'] = [{} for _ in range(len(df))]\n",
    "else:\n",
    "    df['feature_cache'] = df['feature_cache'].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "\n",
    "new_features_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    text = row.get('text', '') or ''\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    seed = int(idx) if isinstance(idx, (int, np.integer)) else abs(hash(idx)) & 0xffffffff\n",
    "\n",
    "    lexical_keys = {\n",
    "        'n_tokens': len(tokens),\n",
    "        'n_types': len(set(tokens)),\n",
    "        'ttr': type_token_ratio(tokens),\n",
    "        'hapax_5k': hapax_in_sample(tokens, sample_size=5000, seed=seed),\n",
    "        'mtld': mtld_calc(tokens),\n",
    "        'sent_len_std': sentence_length_std(text),\n",
    "    }\n",
    "\n",
    "    syntactic = analyze_syntax(text)\n",
    "    synt_keys = {\n",
    "        'adj_noun_ratio': syntactic.get('adj_noun_ratio', 0),\n",
    "        'tree_depth': syntactic.get('tree_depth', 0),\n",
    "        'fk_grade': syntactic.get('fk_grade', 0),\n",
    "        'discourse_density_per_100_words': syntactic.get('discourse_density_per_100_words', 0)\n",
    "    }\n",
    "\n",
    "    fc = dict(row['feature_cache']) if isinstance(row['feature_cache'], dict) else {}\n",
    "\n",
    "    for k, v in lexical_keys.items():\n",
    "        if k not in fc:\n",
    "            fc[k] = v\n",
    "\n",
    "    for k, v in synt_keys.items():\n",
    "        if k not in fc:\n",
    "            fc[k] = v\n",
    "\n",
    "    if 'function_word_pca' not in fc:\n",
    "        fc['function_word_pca'] = {\n",
    "            'dim1': float(row.get('function_word_pca_dim1', 0)),\n",
    "            'dim2': float(row.get('function_word_pca_dim2', 0))\n",
    "        }\n",
    "\n",
    "    for name in PUNCT_NAMES:\n",
    "        col = f'punct_{name}'\n",
    "        if col not in fc:\n",
    "            fc[col] = float(row.get(col, 0))\n",
    "\n",
    "    new_features_list.append(fc)\n",
    "\n",
    "df['feature_cache'] = new_features_list\n",
    "\n",
    "drop_cols = ['function_word_pca_dim1', 'function_word_pca_dim2'] + [f'punct_{name}' for name in PUNCT_NAMES]\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "print(\"Feature cache updated for all rows\")\n",
    "print(\"Lexical: n_tokens, n_types, ttr, hapax_5k, mtld, sent_len_std\")\n",
    "print(\"Syntactic: adj_noun_ratio, tree_depth, fk_grade, discourse_density_per_100_words\")\n",
    "print(\"Stylometric: function_word_pca (dim1, dim2)\")\n",
    "print(\"Punctuation: 11 punct_* features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710976eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote enriched parquet to: w:\\Programming\\PKOG\\preprecogclean\\data_analysis\\data.parquet\n",
      "\n",
      "Sample feature_cache:\n",
      "{\n",
      "  \"author\": \"gemini-3-flash-preview\",\n",
      "  \"avg_sent_length\": 19.25,\n",
      "  \"book_title\": null,\n",
      "  \"persona_mimicked\": \"Imposter_James\",\n",
      "  \"word_count\": 154,\n",
      "  \"n_tokens\": 154,\n",
      "  \"n_types\": 94,\n",
      "  \"ttr\": 0.6103896103896104,\n",
      "  \"hapax_5k\": 76,\n",
      "  \"mtld\": 58.48877236381374,\n",
      "  \"sent_len_std\": 7.578753195612059,\n",
      "  \"adj_noun_ratio\": 0.4482758620689655,\n",
      "  \"tree_depth\": 6.5,\n",
      "  \"fk_grade\": 9.157759740259742,\n",
      "  \"discourse_density_per_100_words\": 0.0,\n",
      "  \"function_word_pca\": {\n",
      "    \"dim1\": -0.12540206561297113,\n",
      "    \"dim2\": -0.01780129978267521\n",
      "  },\n",
      "  \"punct_semicolon\": 2.4242424242424243,\n",
      "  \"punct_colon\": 0.0,\n",
      "  \"punct_exclamation\": 0.0,\n",
      "  \"punct_question\": 0.0,\n",
      "  \"punct_hyphen\": 0.0,\n",
      "  \"punct_emdash\": 0.0,\n",
      "  \"punct_asterisk\": 0.0,\n",
      "  \"punct_apos\": 0.0,\n",
      "  \"punct_apos_curly\": 0.0,\n",
      "  \"punct_paren_open\": 0.0,\n",
      "  \"punct_quote\": 7.2727272727272725\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "out_path = Path.cwd().joinpath('data.parquet')\n",
    "df.to_parquet(out_path, index=False)\n",
    "print(f\"Wrote enriched parquet to: {out_path}\")\n",
    "\n",
    "import json\n",
    "sample_fc = df.sample(1).iloc[0]['feature_cache']\n",
    "print('\\nSample feature_cache:')\n",
    "print(json.dumps(sample_fc, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
