{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82aa0a1",
   "metadata": {},
   "source": [
    "# Clause Assembler\n",
    "Post-processing pipeline for GA output. Splits AI text into clauses via Gemini, applies humanizing transforms (comma insertion, parallel structure breaking, punctuation randomization via monte carlo), writes to output.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a59dc0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eca512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orbi8\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.api_core import exceptions\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "    sys.stderr.reconfigure(encoding='utf-8')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95fe7e",
   "metadata": {},
   "source": [
    "Config -- paths, API key, model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f76e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"Clause_Assembler.ipynb\")) if os.path.exists(\"Clause_Assembler.ipynb\") else current_dir\n",
    "\n",
    "INPUT_FILE = os.path.join(notebook_dir, \"input.txt\")\n",
    "OUTPUT_FILE = os.path.join(notebook_dir, \"output.txt\")\n",
    "\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.dirname(notebook_dir)))\n",
    "ENV_PATH = os.path.join(project_root, \".env\")\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file!\")\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c56bf2",
   "metadata": {},
   "source": [
    "Gemini API call w/ retry + rate limit handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1e878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_api(prompt, max_retries=5, return_json=False):\n",
    "    attempt = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            config = {}\n",
    "            if return_json:\n",
    "                config[\"response_mime_type\"] = \"application/json\"\n",
    "            \n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME, \n",
    "                contents=prompt,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            if hasattr(response, \"text\") and response.text:\n",
    "                result = response.text\n",
    "            else:\n",
    "                result = str(response)\n",
    "            \n",
    "            if return_json:\n",
    "                try:\n",
    "                    return json.loads(result)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON parse error: {e}\")\n",
    "                    print(f\"Raw response: {result[:200]}...\")\n",
    "                    return None\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            err_str = str(e)\n",
    "            \n",
    "            if \"429\" in err_str or \"ResourceExhausted\" in str(type(e)) or \"Quota exceeded\" in err_str:\n",
    "                wait_time = 30.0\n",
    "                print(f\"Rate limit hit. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            attempt += 1\n",
    "            if attempt >= max_retries:\n",
    "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            \n",
    "            backoff_time = (2 ** attempt) + np.random.uniform(0, 1)\n",
    "            print(f\"API error: {e}. Retrying in {backoff_time:.1f}s...\")\n",
    "            time.sleep(backoff_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a0345",
   "metadata": {},
   "source": [
    "Thought unit extraction -- Gemini parses text into weighted clauses w/ prosodic resolution flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f03190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thought_units(paragraph):\n",
    "    prompt = f\"\"\"Analyze this paragraph and break it into atomic \"Thought Units\" (clauses or phrases that hold a single piece of information).\n",
    "\n",
    "For each unit, provide:\n",
    "1. \"text\": The exact substring (preserve original wording)\n",
    "2. \"weight\": Integer 1-10 representing Mental Load or information density\n",
    "   - High (8-10): Complex concepts, dense information, technical terms\n",
    "   - Medium (4-7): Standard clauses with moderate complexity\n",
    "   - Low (1-3): Simple connectors, transitions, basic statements\n",
    "3. \"resolves_thought\": Boolean - does this clause sound like the end of an idea?\n",
    "   - True: Falling intonation, feels complete, prosodic resolution\n",
    "   - False: Rising or level intonation, feels unresolved, needs continuation\n",
    "\n",
    "PARAGRAPH:\n",
    "{paragraph}\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "[\n",
    "  {{\"text\": \"clause text here\", \"weight\": 5, \"resolves_thought\": false}},\n",
    "  {{\"text\": \"next clause\", \"weight\": 7, \"resolves_thought\": true}}\n",
    "]\n",
    "\n",
    "Do not include any explanation, just the JSON array.\"\"\"\n",
    "\n",
    "    result = call_gemini_api(prompt, return_json=True)\n",
    "    \n",
    "    if result is None or not isinstance(result, list):\n",
    "        print(\"Failed to parse thought units, using fallback\")\n",
    "        sentences = paragraph.replace('?', '.').replace('!', '.').split('.')\n",
    "        return [\n",
    "            {\"text\": s.strip(), \"weight\": 5, \"resolves_thought\": True} \n",
    "            for s in sentences if s.strip()\n",
    "        ]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a25b8e",
   "metadata": {},
   "source": [
    "Transforms: thinking commas, parallel structure breaking, log-normal monte carlo punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92083164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thinking_commas(text):\n",
    "    words = text.split()\n",
    "    if len(words) <= 3:\n",
    "        return text\n",
    "    \n",
    "    if np.random.random() > 0.6:\n",
    "        return text\n",
    "    \n",
    "    num_commas = np.random.choice([1, 2, 3], p=[0.5, 0.3, 0.2])\n",
    "    \n",
    "    for _ in range(num_commas):\n",
    "        if len(words) <= 3:\n",
    "            break\n",
    "        insert_pos = np.random.randint(1, len(words) - 1)\n",
    "        words[insert_pos] = ', ' + words[insert_pos]\n",
    "    \n",
    "    return ' '.join(words).replace(' , ', ', ')\n",
    "\n",
    "\n",
    "def break_parallel_structure(text):\n",
    "    if ', and' in text or ', or' in text:\n",
    "        if np.random.random() < 0.5:\n",
    "            text = text.replace(', and', '. and', 1)\n",
    "        if ', or' in text and np.random.random() < 0.5:\n",
    "            text = text.replace(', or', '. or', 1)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def monte_carlo_punctuation(thought_units, casual_mode=True):\n",
    "    if casual_mode:\n",
    "        MU = 1.8\n",
    "        SIGMA = 1.2\n",
    "    else:\n",
    "        MU = 2.8\n",
    "        SIGMA = 0.8\n",
    "    \n",
    "    reconstructed_text = \"\"\n",
    "    current_sentence_load = 0\n",
    "    current_word_count = 0\n",
    "    \n",
    "    target_load = np.random.lognormal(MU, SIGMA)\n",
    "    \n",
    "    sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "    \n",
    "    for i, unit in enumerate(thought_units):\n",
    "        text = unit['text'].strip()\n",
    "        weight = unit['weight']\n",
    "        resolves = unit['resolves_thought']\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        text = text.rstrip('.,;:!?')\n",
    "        \n",
    "        if casual_mode and len(text.split()) > 4:\n",
    "            if np.random.random() < 0.6:\n",
    "                text = add_thinking_commas(text)\n",
    "        \n",
    "        text = break_parallel_structure(text)\n",
    "        \n",
    "        reconstructed_text += text\n",
    "        current_sentence_load += weight\n",
    "        current_word_count += len(text.split())\n",
    "        \n",
    "        if i == len(thought_units) - 1:\n",
    "            if casual_mode and np.random.random() > 0.5:\n",
    "                pass\n",
    "            else:\n",
    "                reconstructed_text += \".\"\n",
    "            break\n",
    "        \n",
    "        if sentence_mode == 'fragment':\n",
    "            if current_word_count >= 2 and np.random.random() < 0.6:\n",
    "                reconstructed_text += \". \"\n",
    "                current_sentence_load = 0\n",
    "                current_word_count = 0\n",
    "                target_load = np.random.lognormal(MU, SIGMA)\n",
    "                sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "                continue\n",
    "        \n",
    "        elif sentence_mode == 'runon':\n",
    "            if current_word_count >= 15 and current_word_count <= 30:\n",
    "                if np.random.random() < 0.2:\n",
    "                    reconstructed_text += \". \"\n",
    "                    current_sentence_load = 0\n",
    "                    current_word_count = 0\n",
    "                    target_load = np.random.lognormal(MU, SIGMA)\n",
    "                    sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "                    continue\n",
    "            \n",
    "            if current_word_count < 40:\n",
    "                reconstructed_text += \", \"\n",
    "                continue\n",
    "            elif current_word_count >= 40 and current_word_count < 65:\n",
    "                if np.random.random() < 0.25:\n",
    "                    reconstructed_text += \". \"\n",
    "                    current_sentence_load = 0\n",
    "                    current_word_count = 0\n",
    "                    target_load = np.random.lognormal(MU, SIGMA)\n",
    "                    sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "                else:\n",
    "                    reconstructed_text += \", \"\n",
    "                continue\n",
    "            else:\n",
    "                reconstructed_text += \". \"\n",
    "                current_sentence_load = 0\n",
    "                current_word_count = 0\n",
    "                target_load = np.random.lognormal(MU, SIGMA)\n",
    "                sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "                continue\n",
    "        \n",
    "        load_ratio = current_sentence_load / target_load\n",
    "        \n",
    "        p_stop = 1 / (1 + math.exp(-6 * (load_ratio - 0.7)))\n",
    "        \n",
    "        if resolves:\n",
    "            p_stop += 0.1\n",
    "        else:\n",
    "            p_stop -= 0.05\n",
    "        \n",
    "        if np.random.random() < 0.15:\n",
    "            p_stop += 0.6\n",
    "        \n",
    "        p_stop = max(0.0, min(1.0, p_stop))\n",
    "        \n",
    "        monte_carlo_roll = np.random.random()\n",
    "        \n",
    "        if monte_carlo_roll < p_stop:\n",
    "            reconstructed_text += \". \"\n",
    "            \n",
    "            current_sentence_load = 0\n",
    "            current_word_count = 0\n",
    "            target_load = np.random.lognormal(MU, SIGMA)\n",
    "            sentence_mode = np.random.choice(['fragment', 'runon', 'normal'], p=[0.25, 0.25, 0.5])\n",
    "            \n",
    "        else:\n",
    "            if current_sentence_load > (target_load * 0.25):\n",
    "                reconstructed_text += \", \"\n",
    "            else:\n",
    "                if np.random.random() < 0.25:\n",
    "                    reconstructed_text += \", \"\n",
    "                else:\n",
    "                    reconstructed_text += \" \"\n",
    "    \n",
    "    return reconstructed_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d10c73",
   "metadata": {},
   "source": [
    "Main workflow -- deconstruct, reassemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11cc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize_workflow(input_text, casual_mode=True, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"Input length: {len(input_text)} chars\")\n",
    "        print(f\"Mode: {'casual' if casual_mode else 'formal'}\")\n",
    "    \n",
    "    thought_units = get_thought_units(input_text)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {len(thought_units)} thought units\")\n",
    "        for i, unit in enumerate(thought_units[:3]):\n",
    "            print(f\"  [{i+1}] \\\"{unit['text'][:50]}...\\\"\")\n",
    "            print(f\"      Weight: {unit['weight']}/10, Resolves: {unit['resolves_thought']}\")\n",
    "        if len(thought_units) > 3:\n",
    "            print(f\"  ... and {len(thought_units) - 3} more\")\n",
    "    \n",
    "    output_text = monte_carlo_punctuation(thought_units, casual_mode=casual_mode)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Reassembled {len(output_text)} chars\")\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a8fed",
   "metadata": {},
   "source": [
    "Load input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12908028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1006 chars from w:\\Programming\\PKOG\\preprecogclean\\imposter\\clauses_assemble\\input.txt\n",
      "Furthermore, the existential dimension of truth and reality cannot be ignored. For thinkers like Søren Kierkegaard, truth is not merely a matter of factual accuracy but of lived authenticity. He famously asserted that \"subjectivity is truth,\" implying that the most important truths are those that a person is willing to live and die for. This existential truth is found in the alignment of one’s actions with their deepest convictions, creating a sense of internal reality that provides meaning in an otherwise indifferent universe. In this sense, the quest for truth is not just an academic exercise but a moral imperative. To seek the truth is to honor the reality of our existence and to refuse the comfort of convenient delusions. Ultimately, the journey toward understanding truth and reality is the defining narrative of the human species. We are the only creatures known to ask \"why\" and \"is it true?\" This inquiry drives our scientific discoveries, our artistic expressions, and our legal systems.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        input_text = f.read().strip()\n",
    "    print(f\"Loaded {len(input_text)} chars from {INPUT_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    input_text = \"\"\"Furthermore, the existential dimension of truth and reality cannot be ignored. For thinkers like Søren Kierkegaard, truth is not merely a matter of factual accuracy but of lived authenticity. He famously asserted that \"subjectivity is truth,\" implying that the most important truths are those that a person is willing to live and die for.\"\"\"\n",
    "    with open(INPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(input_text)\n",
    "    print(\"Created sample input file\")\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8743a",
   "metadata": {},
   "source": [
    "Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c010105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1006 chars\n",
      "Mode: casual\n",
      "Detected 23 thought units\n",
      "  [1] \"Furthermore,...\"\n",
      "      Weight: 2/10, Resolves: False\n",
      "  [2] \"the existential dimension of truth and reality can...\"\n",
      "      Weight: 8/10, Resolves: True\n",
      "  [3] \"For thinkers like Søren Kierkegaard,...\"\n",
      "      Weight: 4/10, Resolves: False\n",
      "  ... and 20 more\n",
      "Reassembled 1035 chars\n",
      "\n",
      "OUTPUT:\n",
      "------------------------------------------------------------\n",
      "Furthermore, the existential dimension of truth and reality cannot be ignored, For thinkers like Søren Kierkegaard. truth is not merely a matter of factual accuracy. but of lived authenticity, He, famously asserted that \"subjectivity is truth,\". implying that the most important truths are those, that a person is willing to, live and die for. This existential truth is found in the alignment of one’s actions with their deepest convictions, creating a sense of internal reality. that provides, meaning in an, , otherwise indifferent universe. In this sense. the quest for truth is not just an academic exercise. but a moral imperative, To seek the, truth is to honor the reality, of our existence, and to refuse, , the comfort of, convenient delusions. Ultimately, the journey toward understanding truth and reality is, , the, defining narrative of the human species, We are the only creatures known, to, ask \"why\" and, \"is, it true?\", This inquiry drives, our, scientific discoveries, our artistic expressions, and our legal systems.\n"
     ]
    }
   ],
   "source": [
    "output_text = humanize_workflow(input_text, casual_mode=True, verbose=True)\n",
    "\n",
    "print(\"\\nOUTPUT:\")\n",
    "print(\"-\" * 60)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d142dbc",
   "metadata": {},
   "source": [
    "Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fb3343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: w:\\Programming\\PKOG\\preprecogclean\\imposter\\clauses_assemble\\output.txt\n",
      "  Length: 1035 characters\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(output_text)\n",
    "    print(f\"Output saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"  Length: {len(output_text)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf1b09",
   "metadata": {},
   "source": [
    "Punctuation stats -- compare original vs humanized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8d86ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL:\n",
      "  Total characters: 1006\n",
      "  Sentences: 8\n",
      "  Periods: 8\n",
      "  Commas: 8\n",
      "  Avg words/sentence: 20.2\n",
      "  Sentence length std dev: 5.6\n",
      "  Min/Max sentence length: 11/29 words\n",
      "\n",
      "HUMANIZED:\n",
      "  Total characters: 1035\n",
      "  Sentences: 10\n",
      "  Periods: 10\n",
      "  Commas: 32\n",
      "  Avg words/sentence: 16.5\n",
      "  Sentence length std dev: 10.8\n",
      "  Min/Max sentence length: 3/43 words\n"
     ]
    }
   ],
   "source": [
    "def analyze_punctuation(text, label=\"Text\"):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    words_per_sentence = [len(s.split()) for s in sentences]\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Total characters: {len(text)}\")\n",
    "    print(f\"  Sentences: {len(sentences)}\")\n",
    "    print(f\"  Periods: {text.count('.')}\")\n",
    "    print(f\"  Commas: {text.count(',')}\")\n",
    "    \n",
    "    if words_per_sentence:\n",
    "        print(f\"  Avg words/sentence: {np.mean(words_per_sentence):.1f}\")\n",
    "        print(f\"  Sentence length std dev: {np.std(words_per_sentence):.1f}\")\n",
    "        print(f\"  Min/Max sentence length: {min(words_per_sentence)}/{max(words_per_sentence)} words\")\n",
    "\n",
    "analyze_punctuation(input_text, \"ORIGINAL\")\n",
    "analyze_punctuation(output_text, \"HUMANIZED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30ebfd",
   "metadata": {},
   "source": [
    "Sentence length distribution -- fragment/normal/run-on breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3849e128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL - SENTENCE LENGTH DISTRIBUTION\n",
      "============================================================\n",
      "Total sentences: 8\n",
      "  Fragments (<=8 words):   0 (  0.0%) - []\n",
      "  Normal (9-34 words):    8 (100.0%)\n",
      "  Run-ons (>=35 words):    0 (  0.0%) - []\n",
      "  EXTREMES TOTAL:         0 (  0.0%)\n",
      "\n",
      "  Min: 11 | Max: 29 | Mean: 20.2 | Std: 5.6\n",
      "Below target: 0.0% extremes (goal: 40%+)\n",
      "\n",
      "HUMANIZED - SENTENCE LENGTH DISTRIBUTION\n",
      "============================================================\n",
      "Total sentences: 10\n",
      "  Fragments (<=8 words):   1 ( 10.0%) - [3]\n",
      "  Normal (9-34 words):    8 ( 80.0%)\n",
      "  Run-ons (>=35 words):    1 ( 10.0%) - [43]\n",
      "  EXTREMES TOTAL:         2 ( 20.0%)\n",
      "\n",
      "  Min: 3 | Max: 43 | Mean: 16.5 | Std: 10.8\n",
      "Below target: 20.0% extremes (goal: 40%+)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_sentence_distribution(text, label=\"Text\"):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    lengths = [len(s.split()) for s in sentences]\n",
    "    \n",
    "    if not lengths:\n",
    "        print(f\"{label}: No sentences found\")\n",
    "        return\n",
    "    \n",
    "    fragments = [l for l in lengths if l <= 8]\n",
    "    runons = [l for l in lengths if l >= 35]\n",
    "    normal = [l for l in lengths if 8 < l < 35]\n",
    "    \n",
    "    total = len(lengths)\n",
    "    pct_fragments = (len(fragments) / total) * 100\n",
    "    pct_runons = (len(runons) / total) * 100\n",
    "    pct_extremes = pct_fragments + pct_runons\n",
    "    \n",
    "    print(f\"\\n{label} - SENTENCE LENGTH DISTRIBUTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"  Fragments (<=8 words):  {len(fragments):2d} ({pct_fragments:5.1f}%) - {fragments}\")\n",
    "    print(f\"  Normal (9-34 words):   {len(normal):2d} ({len(normal)/total*100:5.1f}%)\")\n",
    "    print(f\"  Run-ons (>=35 words):   {len(runons):2d} ({pct_runons:5.1f}%) - {runons}\")\n",
    "    print(f\"  EXTREMES TOTAL:        {len(fragments)+len(runons):2d} ({pct_extremes:5.1f}%)\")\n",
    "    print(f\"\\n  Min: {min(lengths)} | Max: {max(lengths)} | Mean: {np.mean(lengths):.1f} | Std: {np.std(lengths):.1f}\")\n",
    "    \n",
    "    if pct_extremes >= 40:\n",
    "        print(f\"Target met: {pct_extremes:.1f}% extremes (goal: 40%+)\")\n",
    "    else:\n",
    "        print(f\"Below target: {pct_extremes:.1f}% extremes (goal: 40%+)\")\n",
    "    \n",
    "    return pct_extremes\n",
    "\n",
    "analyze_sentence_distribution(input_text, \"ORIGINAL\")\n",
    "analyze_sentence_distribution(output_text, \"HUMANIZED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be594a2",
   "metadata": {},
   "source": [
    "Generate 3 variants to show monte carlo variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4522dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARIANT 1:\n",
      "------------------------------------------------------------\n",
      "Furthermore, the existential dimension, of truth and reality cannot be ignored, For, thinkers, like Søren Kierkegaard, truth is not merely a matter of factual accuracy. but of lived authenticity. He famously asserted that \"subjectivity, is truth,\", implying that the most important truths are those, that a person is willing to live and die for. This existential truth is found in the alignment of one’s actions with their deepest convictions. creating a sense of internal reality. that provides meaning in an otherwise indifferent universe. In this sense, the quest for truth is not just an academic exercise, but a moral imperative, To seek the truth is to honor the reality of our existence, and to refuse the comfort of convenient delusions, Ultimately, the journey toward understanding truth. and reality, is the defining narrative of the human species. We are the only creatures known to ask \"why\" and \"is it true?\", This inquiry drives our scientific discoveries. our artistic expressions. and our legal systems\n",
      "Periods: 10 | Commas: 18\n",
      "\n",
      "VARIANT 2:\n",
      "------------------------------------------------------------\n",
      "Furthermore, the existential dimension of truth and reality cannot be ignored, For thinkers like Søren Kierkegaard, truth, is not, merely a matter of factual accuracy, but of lived authenticity, He famously asserted that \"subjectivity is truth,\", implying that the most important truths are those, that a person is willing to live and die for. This existential truth is found in the alignment of one’s actions with their deepest convictions. creating a sense of internal reality. that provides meaning in an otherwise indifferent universe. In this sense, the quest for truth is not just an academic exercise. but a moral imperative. To seek the truth is to honor the reality of our existence. and to refuse the comfort of convenient delusions, Ultimately, the journey toward understanding truth and reality, is the defining narrative of the human species. We are the only creatures known to ask \"why\" and \"is it true?\". This inquiry drives our, scientific discoveries, our artistic expressions, and our legal systems.\n",
      "Periods: 10 | Commas: 17\n",
      "\n",
      "VARIANT 3:\n",
      "------------------------------------------------------------\n",
      "Furthermore, the, existential dimension of truth and reality cannot be ignored, For thinkers like Søren Kierkegaard, truth is not merely a matter of factual accuracy. but of lived authenticity, He famously asserted that \"subjectivity is truth,\", implying that the most important truths are those, that a person is willing to live and die for, This existential truth is found, in the, alignment of one’s actions, with their deepest convictions, creating a sense of internal reality. that, provides meaning in an otherwise, indifferent universe. In this sense. the quest, for, truth is not just an academic exercise, but a moral imperative. To seek the truth is to honor the reality of our existence. and to refuse the comfort of convenient delusions. Ultimately, the journey toward understanding truth and reality, is the defining narrative of the human species. We are the only creatures known to ask \"why\" and \"is it true?\". This inquiry drives our scientific discoveries, our artistic expressions, and our legal systems\n",
      "Periods: 9 | Commas: 22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thought_units = get_thought_units(input_text)\n",
    "\n",
    "for i in range(3):\n",
    "    variant = monte_carlo_punctuation(thought_units, casual_mode=True)\n",
    "    \n",
    "    print(f\"VARIANT {i+1}:\")\n",
    "    print('-'*60)\n",
    "    print(variant)\n",
    "    print(f\"Periods: {variant.count('.')} | Commas: {variant.count(',')}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
